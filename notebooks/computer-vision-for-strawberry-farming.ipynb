{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10785700,"sourceType":"datasetVersion","datasetId":6693136}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport math\nimport random\nimport glob\nfrom typing import Tuple, List\nfrom tqdm import tqdm\n\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torchvision.io import read_image\nfrom torchvision.ops import box_convert\nfrom torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn.functional as F\n\nprint('Libraries imported.')\n\n# ===============================\n# Configuration / Hyperparameters\n# ===============================\nDATA_DIR = \"/kaggle/input/strawdi/data/StrawDI_Db1\"  # Path to dataset\nVIDEO_PATH = \"/kaggle/input/strawdi/data/test.mp4\"   # Path to video\nTRAIN_EPOCHS = 1\nLR = 1e-3\nBATCH_SIZE = 4\nDEVICE = \"cuda\"  # or \"cpu\"\n\n\n# ---------------------------------------------------------\n# 1. Dataset Definition\n# ---------------------------------------------------------\n\nclass StrawberriesDataset(Dataset):\n    \"\"\"\n    A custom dataset for Strawberry detection+segmentation.\n\n    Assumes structure:\n      root/\n         train/img/*.png\n         train/label/*.png\n         val/img/*.png\n         val/label/*.png\n         test/img/*.png\n         test/label/*.png\n\n    Each label image has integer IDs marking strawberries (0 is background).\n    \"\"\"\n\n    def __init__(self, root: str, split: str = \"train\", transform=None):\n        super().__init__()\n        self.root = root\n        self.split = split\n        self.transform = transform\n\n        img_dir = os.path.join(root, split, \"img\")\n        mask_dir = os.path.join(root, split, \"label\")\n\n        self.img_paths = sorted(glob.glob(os.path.join(img_dir, \"*\")))\n        self.label_paths = sorted(glob.glob(os.path.join(mask_dir, \"*\")))\n\n        assert len(self.img_paths) == len(self.label_paths), \\\n            \"Mismatch between image and mask count.\"\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.img_paths[idx]\n        mask_path = self.label_paths[idx]\n\n        # Read with torchvision\n        img = read_image(img_path).float() / 255.0  # shape: [C,H,W], range: [0,1]\n        mask = read_image(mask_path)                # shape: [1,H,W] with integer IDs\n\n        # Convert from 1-channel integer ID to multi-class (BG + 1 class for strawberry)\n        is_straw = mask[0] > 0  # shape [H,W]\n        # semantic = 2 channels: [bg, strawberry]\n        semantic = torch.stack([~is_straw, is_straw], dim=0).float()\n\n        # Build bounding boxes\n        unique_ids = torch.unique(mask[0])  # e.g., [0,1,2,...]\n        unique_ids = unique_ids[unique_ids > 0]  # exclude background\n\n        if len(unique_ids) == 0:\n            # No strawberries\n            boxes = torch.zeros((0, 6))  # shape [N,6] => [img_idx, cls, cx,cy,w,h]\n        else:\n            # Build multi-channel mask per ID:\n            all_obj_masks = torch.stack([(mask[0] == u) for u in unique_ids], dim=0)\n            # Convert to boxes\n            raw_boxes_xyxy = torchvision.ops.masks_to_boxes(all_obj_masks)\n            # XYXY -> CXCYWH\n            raw_boxes_cxcywh = box_convert(raw_boxes_xyxy, in_fmt=\"xyxy\", out_fmt=\"cxcywh\")\n            # Normalize\n            H, W = img.shape[1], img.shape[2]\n            raw_boxes_cxcywh[:, [0]] /= float(W)\n            raw_boxes_cxcywh[:, [1]] /= float(H)\n            raw_boxes_cxcywh[:, [2]] /= float(W)\n            raw_boxes_cxcywh[:, [3]] /= float(H)\n\n            box_num = raw_boxes_cxcywh.shape[0]\n            boxes = torch.zeros((box_num, 6))\n            # class=0 for strawberry in all rows\n            boxes[:, 1] = 0.0\n            boxes[:, 2:] = raw_boxes_cxcywh\n\n        if self.transform is not None:\n            img, semantic = self.transform(img, semantic)\n\n        return img, (semantic, boxes)\n\n    @staticmethod\n    def collate_fn(batch):\n        \"\"\"\n        Custom collate to handle variable #boxes.\n        Output:\n          images: [B,3,H,W]\n          segs:   [B,2,H,W]\n          boxes:  [sum_all_boxes, 6]\n        \"\"\"\n        imgs = []\n        segs = []\n        all_boxes = []\n\n        for i, (img, (seg, boxes)) in enumerate(batch):\n            imgs.append(img)\n            segs.append(seg)\n\n            # Fill in \"image index\" for each box\n            if len(boxes) > 0:\n                boxes_with_idx = boxes.clone()\n                boxes_with_idx[:, 0] = i\n                all_boxes.append(boxes_with_idx)\n\n        # Stack\n        imgs_t = torch.stack(imgs, dim=0)\n        segs_t = torch.stack(segs, dim=0)\n\n        if len(all_boxes) > 0:\n            boxes_t = torch.cat(all_boxes, dim=0)\n        else:\n            # no boxes at all\n            boxes_t = torch.zeros((0, 6))\n\n        return imgs_t, (segs_t, boxes_t)\n\n\n# ---------------------------------------------------------\n# 2. Simple Augmentations\n# ---------------------------------------------------------\n\nclass ResizeAndPad:\n    \"\"\"\n    Resize the image & label to a fixed shape using bilinear (for image) \n    and nearest (for mask).\n    \"\"\"\n    def __init__(self, out_h=480, out_w=640):\n        self.out_h = out_h\n        self.out_w = out_w\n\n    def __call__(self, img, seg):\n        img_r = torchvision.transforms.functional.resize(\n            img, (self.out_h, self.out_w), antialias=True\n        )\n        seg_r = torchvision.transforms.functional.resize(\n            seg, (self.out_h, self.out_w), torchvision.transforms.InterpolationMode.NEAREST\n        )\n        return img_r, seg_r\n\n\n# ---------------------------------------------------------\n# 3. Model\n# ---------------------------------------------------------\n\nclass SimpleBackbone(nn.Module):\n    def __init__(self, in_ch=3, out_ch=64):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_ch, 32, kernel_size=3, stride=2, padding=1)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.act = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(32, out_ch, kernel_size=3, stride=2, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_ch)\n\n    def forward(self, x):\n        x = self.act(self.bn1(self.conv1(x)))\n        x = self.act(self.bn2(self.conv2(x)))\n        return x  # [B, out_ch, H/4, W/4]\n\n\nclass SegHead(nn.Module):\n    \"\"\"Up-conv to produce 2-channel seg (bg, straw).\"\"\"\n    def __init__(self, in_ch=64, out_ch=2):\n        super().__init__()\n        self.up1 = nn.ConvTranspose2d(in_ch, in_ch//2, kernel_size=2, stride=2)\n        self.bn1 = nn.BatchNorm2d(in_ch//2)\n        self.act = nn.ReLU(inplace=True)\n\n        self.up2 = nn.ConvTranspose2d(in_ch//2, out_ch, kernel_size=2, stride=2)\n\n    def forward(self, x):\n        x = self.act(self.bn1(self.up1(x)))\n        x = self.up2(x)\n        return x  # [B,2,H,W]\n\n\nclass DetectHead(nn.Module):\n    \"\"\"Naive detection head -> [B, A*(5+num_classes), H/4, W/4].\"\"\"\n    def __init__(self, in_ch=64, num_anchors=3, num_classes=1):\n        super().__init__()\n        self.num_anchors = num_anchors\n        self.num_classes = num_classes\n        self.out_filters = 5 + num_classes  # cx,cy,w,h,obj + class\n\n        self.conv = nn.Conv2d(\n            in_ch, self.num_anchors * self.out_filters,\n            kernel_size=1, stride=1, padding=0\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass StrawModel(nn.Module):\n    \"\"\"Shared backbone => segmentation head => detection head.\"\"\"\n    def __init__(self, num_classes=1):\n        super().__init__()\n        self.anchors = torch.tensor([[10,13],[16,30],[33,50]], dtype=torch.float32)\n        self.backbone = SimpleBackbone(3, 64)\n        self.seg_head = SegHead(in_ch=64, out_ch=2)\n        self.det_head = DetectHead(in_ch=64, num_anchors=3, num_classes=num_classes)\n\n    def forward(self, x):\n        feat = self.backbone(x)\n        seg_out = self.seg_head(feat)\n        det_out = self.det_head(feat)\n        return seg_out, det_out\n\n\n# ---------------------------------------------------------\n# 4. Losses\n# ---------------------------------------------------------\n\ndef segmentation_loss(pred_seg, true_seg):\n    bce = nn.BCEWithLogitsLoss()\n    loss_seg = bce(pred_seg, true_seg)\n\n    # Simple IoU for logging\n    with torch.no_grad():\n        pred_label = torch.argmax(pred_seg, dim=1)  # [B,H,W], 0 or 1\n        true_label = torch.argmax(true_seg, dim=1)\n        inter = torch.logical_and(pred_label == 1, true_label == 1).sum().item()\n        union = (torch.logical_or(pred_label == 1, true_label == 1)).sum().item()\n        iou = inter / (union + 1e-6) if union > 0 else 1.0\n\n    return loss_seg, iou\n\n\ndef detection_loss(pred_det, true_boxes, device=\"cpu\"):\n    \"\"\"\n    Toy YOLO-like detection loss for demonstration, not a full YOLO.\n    \"\"\"\n    bce = nn.BCEWithLogitsLoss()\n    mse = nn.MSELoss()\n\n    B, _, H, W = pred_det.shape\n    A = 3\n    out_ch = 5 + 1\n    pred_det = pred_det.view(B, A, out_ch, H, W)\n\n    obj_target = torch.zeros_like(pred_det[:, :, 4])  # shape [B,A,H,W]\n    box_target = torch.zeros_like(pred_det[:, :, 0:4])# shape [B,A,4,H,W]\n    cls_target = torch.zeros_like(pred_det[:, :, 5:]) # shape [B,A,1,H,W]\n\n    # For each GT box, place it in the nearest cell (cx,cy).\n    for i in range(true_boxes.shape[0]):\n        b_idx = int(true_boxes[i, 0].item())\n        cx, cy, w_b, h_b = true_boxes[i, 2:].tolist()\n\n        gx = int(cx * W)\n        gy = int(cy * H)\n        if not (0 <= gx < W and 0 <= gy < H):\n            continue\n\n        a_idx = 0  # always anchor 0\n        obj_target[b_idx, a_idx, gy, gx] = 1.0\n        box_target[b_idx, a_idx, 0, gy, gx] = cx\n        box_target[b_idx, a_idx, 1, gy, gx] = cy\n        box_target[b_idx, a_idx, 2, gy, gx] = w_b\n        box_target[b_idx, a_idx, 3, gy, gx] = h_b\n        cls_target[b_idx, a_idx, 0, gy, gx] = 1.0\n\n    pred_box = pred_det[:, :, 0:4]\n    pred_obj = pred_det[:, :, 4]\n    pred_cls = pred_det[:, :, 5:]\n\n    l_obj = bce(pred_obj, obj_target)\n    l_box = mse(pred_box, box_target)\n    l_cls = bce(pred_cls, cls_target)\n\n    return l_obj + 0.5*l_box + l_cls\n\n\n# ---------------------------------------------------------\n# 5. Train / Evaluate\n# ---------------------------------------------------------\n\ndef train_model(model, train_loader, val_loader, epochs=2, lr=1e-3, device=\"cpu\"):\n    model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    for epoch in range(1, epochs+1):\n        model.train()\n        total_loss = 0.0\n        total_iou = 0.0\n        count = 0\n\n        for imgs, (sems, boxes) in tqdm(train_loader, desc=f\"Epoch {epoch} Train\"):\n            imgs, sems, boxes = imgs.to(device), sems.to(device), boxes.to(device)\n\n            optimizer.zero_grad()\n            seg_out, det_out = model(imgs)\n\n            # segmentation\n            l_seg, seg_iou = segmentation_loss(seg_out, sems)\n            # detection\n            l_det = detection_loss(det_out, boxes, device=device)\n            # combined\n            loss = l_seg + l_det\n\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            total_iou += seg_iou\n            count += 1\n\n        avg_loss = total_loss / count\n        avg_iou = total_iou / count\n\n        val_loss, val_iou = evaluate(model, val_loader, device=device)\n        print(f\"Epoch [{epoch}/{epochs}] \"\n              f\"Train Loss: {avg_loss:.3f}, SegIOU: {avg_iou:.3f} | \"\n              f\"Val Loss: {val_loss:.3f}, ValSegIOU: {val_iou:.3f}\")\n\n\n@torch.no_grad()\ndef evaluate(model, loader, device=\"cpu\"):\n    model.eval()\n    model.to(device)\n    total_loss = 0.0\n    total_iou = 0.0\n    count = 0\n\n    for imgs, (sems, boxes) in tqdm(loader, desc=\"Evaluating\"):\n        imgs, sems, boxes = imgs.to(device), sems.to(device), boxes.to(device)\n        seg_out, det_out = model(imgs)\n\n        l_seg, seg_iou = segmentation_loss(seg_out, sems)\n        l_det = detection_loss(det_out, boxes, device=device)\n        loss = l_seg + l_det\n\n        total_loss += loss.item()\n        total_iou += seg_iou\n        count += 1\n\n    avg_loss = total_loss / count if count else 0\n    avg_iou = total_iou / count if count else 0\n    return avg_loss, avg_iou\n\n\n# ---------------------------------------------------------\n# 6. Inference & Counting on Video\n# ---------------------------------------------------------\n\n@torch.no_grad()\ndef detect_on_frame(model, frame: torch.Tensor, device=\"cpu\", conf_thresh=0.5):\n    model.eval()\n    inp = frame.permute(2, 0, 1).unsqueeze(0).to(device)\n    seg_out, det_out = model(inp)\n\n    seg_pred = seg_out[0]  # [2,H,W]\n    seg_label = (seg_pred[1] > seg_pred[0]).cpu()\n\n    B, A_times, H_det, W_det = det_out.shape\n    A = 3\n    out_ch = 5 + 1\n    det_out = det_out.view(B, A, out_ch, H_det, W_det)\n\n    boxes_list = []\n\n    for a_idx in range(A):\n        for gy in range(H_det):\n            for gx in range(W_det):\n                obj_logit = det_out[0, a_idx, 4, gy, gx].item()\n                obj_prob = torch.sigmoid(torch.tensor(obj_logit)).item()\n                if obj_prob < conf_thresh:\n                    continue\n                cxyw = det_out[0, a_idx, 0:4, gy, gx]\n                cx = cxyw[0].item() * frame.shape[1]\n                cy = cxyw[1].item() * frame.shape[0]\n                bw = cxyw[2].item() * frame.shape[1]\n                bh = cxyw[3].item() * frame.shape[0]\n\n                x1 = max(cx - bw/2, 0)\n                y1 = max(cy - bh/2, 0)\n                x2 = min(cx + bw/2, frame.shape[1])\n                y2 = min(cy + bh/2, frame.shape[0])\n\n                boxes_list.append([x1, y1, x2, y2])\n\n    if len(boxes_list) == 0:\n        return np.zeros((0,4), dtype=np.float32), seg_label\n    return np.array(boxes_list, dtype=np.float32), seg_label\n\n\ndef iou(b1, b2):\n    x1 = max(b1[0], b2[0])\n    y1 = max(b1[1], b2[1])\n    x2 = min(b1[2], b2[2])\n    y2 = min(b1[3], b2[3])\n    inter_w = max(0, x2 - x1)\n    inter_h = max(0, y2 - y1)\n    inter = inter_w * inter_h\n    area1 = (b1[2] - b1[0]) * (b1[3] - b1[1])\n    area2 = (b2[2] - b2[0]) * (b2[3] - b2[1])\n    union = area1 + area2 - inter + 1e-6\n    return inter / union\n\n\ndef track_and_count(straws_in_frame, tracks, iou_thresh=0.5):\n    updated_tracks = []\n    used_ids = set()\n\n    for sbox in straws_in_frame:\n        best_i = 0\n        best_id = None\n\n        for t in tracks:\n            t_id = t[0]\n            t_box = t[1:]\n            overlap = iou(sbox, t_box)\n            if overlap > best_i:\n                best_i = overlap\n                best_id = t_id\n\n        if best_i > iou_thresh and best_id is not None:\n            updated_tracks.append([best_id, sbox[0], sbox[1], sbox[2], sbox[3]])\n            used_ids.add(best_id)\n        else:\n            new_id = 1 if len(tracks) == 0 else max(t[0] for t in tracks) + 1\n            updated_tracks.append([new_id, sbox[0], sbox[1], sbox[2], sbox[3]])\n            used_ids.add(new_id)\n\n    return updated_tracks\n\n\nimport cv2\nimport torch\nimport numpy as np\n\ndef process_video_and_count(\n    model,\n    video_path,\n    device=\"cpu\",\n    conf_thresh=0.5,\n    iou_thresh=0.5,\n    out_video_path=\"output.mp4\"\n):\n    \"\"\"\n    Open 'video_path' with OpenCV, run detection+tracking on each frame.\n    - Writes an output video file to 'out_video_path', with bounding boxes & IDs drawn.\n    - Prints how many unique strawberries (tracks) were observed.\n\n    No use of cv2.imshow() or cv2.waitKey() to avoid kernel crashes in headless notebooks.\n    \"\"\"\n\n    print(f\"Starting video processing: {video_path}\")\n    \n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        print(f\"Error: Cannot open video {video_path}\")\n        return\n\n    # Prepare video writer for output\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    writer = cv2.VideoWriter(out_video_path, fourcc, fps, (width, height))\n\n    print(f\"Video properties: FPS={fps}, Width={width}, Height={height}\")\n\n    tracks = []  # each element: [track_id, x1, y1, x2, y2]\n    unique_ids_seen = set()\n    frame_count = 0\n\n    while True:\n        ret, frame_bgr = cap.read()\n        if not ret:\n            print(\"End of video reached.\")\n            break\n\n        frame_count += 1\n        print(f\"Processing frame {frame_count}...\")\n\n        # Convert to torch tensor\n        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n        frame_float = frame_rgb.astype(np.float32) / 255.0\n        frame_t = torch.from_numpy(frame_float)  # shape [H, W, 3]\n\n        # Detect bounding boxes & seg mask\n        boxes_np, seg_mask = detect_on_frame(\n            model, frame_t, device=device, conf_thresh=conf_thresh\n        )\n\n        # Track them\n        new_tracks = track_and_count(boxes_np, tracks, iou_thresh=iou_thresh)\n        tracks = new_tracks\n\n        # Update global set of IDs\n        for t in tracks:\n            unique_ids_seen.add(t[0])\n\n        # Draw bounding boxes/IDs on the frame\n        for t in tracks:\n            tid, x1, y1, x2, y2 = t\n            color = (0, 255, 0)  # green\n            cv2.rectangle(frame_bgr, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n            cv2.putText(frame_bgr, f\"ID:{tid}\",\n                        (int(x1), int(y1) - 5),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n\n        # Write the annotated frame to the output video\n        writer.write(frame_bgr)\n\n    # Clean up\n    cap.release()\n    writer.release()\n\n    total_strawberries = len(unique_ids_seen)\n    print(f\"Finished writing '{out_video_path}'.\")\n    print(f\"Total number of unique strawberries tracked in video: {total_strawberries}\")\n\n\n# ---------------------------------------------------------\n# 7. Single-Run Pipeline\n#    => Train => Evaluate => Process video\n# ---------------------------------------------------------\n\n# 1) Prepare dataset + loaders\ntrain_ds = StrawberriesDataset(DATA_DIR, \"train\", transform=ResizeAndPad(480,640))\nval_ds   = StrawberriesDataset(DATA_DIR, \"val\",   transform=ResizeAndPad(480,640))\ntest_ds  = StrawberriesDataset(DATA_DIR, \"test\",  transform=ResizeAndPad(480,640))\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n                          collate_fn=StrawberriesDataset.collate_fn)\nval_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n                          collate_fn=StrawberriesDataset.collate_fn)\ntest_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n                          collate_fn=StrawberriesDataset.collate_fn)\n\nprint(\"Datasets and loaders ready.\")\n\n# 2) Build model\nmodel = StrawModel(num_classes=1)\n\n# 3) Train\n# print(\"Starting training...\")\n# train_model(model, train_loader, val_loader, epochs=TRAIN_EPOCHS, lr=LR, device=DEVICE)\n# torch.save(model.state_dict(), \"straw_model.pth\")\n\n# Load trained model.\nmodel.load_state_dict(torch.load(\"straw_model.pth\",weights_only=False))\nprint(\"Training completed. Model saved to straw_model.pth\")\n\n# 4) Evaluate on test set\nprint(\"Evaluating on test set...\")\nval_loss, val_iou = evaluate(model, test_loader, device=DEVICE)\nprint(f\"[Eval on test split] Loss: {val_loss:.3f}, SegIOU: {val_iou:.3f}\")\n\n# 5) Inference on video\nprint(\"Running inference on video for counting...\")\nprocess_video_and_count(model, VIDEO_PATH, device=DEVICE)\nprint(\"All steps (train -> eval -> video) done!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T17:59:01.514189Z","iopub.execute_input":"2025-02-18T17:59:01.514498Z","iopub.status.idle":"2025-02-18T19:06:16.485808Z","shell.execute_reply.started":"2025-02-18T17:59:01.514474Z","shell.execute_reply":"2025-02-18T19:06:16.484848Z"}},"outputs":[{"name":"stdout","text":"Libraries imported.\nDatasets and loaders ready.\nTraining completed. Model saved to straw_model.pth\nEvaluating on test set...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 50/50 [00:10<00:00,  4.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Eval on test split] Loss: 0.046, SegIOU: 0.749\nRunning inference on video for counting...\nStarting video processing: /kaggle/input/strawdi/data/test.mp4\nVideo properties: FPS=29.966359034709278, Width=1280, Height=720\nProcessing frame 1...\nProcessing frame 2...\nProcessing frame 3...\nProcessing frame 4...\nProcessing frame 5...\nProcessing frame 6...\nProcessing frame 7...\nProcessing frame 8...\nProcessing frame 9...\nProcessing frame 10...\nProcessing frame 11...\nProcessing frame 12...\nProcessing frame 13...\nProcessing frame 14...\nProcessing frame 15...\nProcessing frame 16...\nProcessing frame 17...\nProcessing frame 18...\nProcessing frame 19...\nProcessing frame 20...\nProcessing frame 21...\nProcessing frame 22...\nProcessing frame 23...\nProcessing frame 24...\nProcessing frame 25...\nProcessing frame 26...\nProcessing frame 27...\nProcessing frame 28...\nProcessing frame 29...\nProcessing frame 30...\nProcessing frame 31...\nProcessing frame 32...\nProcessing frame 33...\nProcessing frame 34...\nProcessing frame 35...\nProcessing frame 36...\nProcessing frame 37...\nProcessing frame 38...\nProcessing frame 39...\nProcessing frame 40...\nProcessing frame 41...\nProcessing frame 42...\nProcessing frame 43...\nProcessing frame 44...\nProcessing frame 45...\nProcessing frame 46...\nProcessing frame 47...\nProcessing frame 48...\nProcessing frame 49...\nProcessing frame 50...\nProcessing frame 51...\nProcessing frame 52...\nProcessing frame 53...\nProcessing frame 54...\nProcessing frame 55...\nProcessing frame 56...\nProcessing frame 57...\nProcessing frame 58...\nProcessing frame 59...\nProcessing frame 60...\nProcessing frame 61...\nProcessing frame 62...\nProcessing frame 63...\nProcessing frame 64...\nProcessing frame 65...\nProcessing frame 66...\nProcessing frame 67...\nProcessing frame 68...\nProcessing frame 69...\nProcessing frame 70...\nProcessing frame 71...\nProcessing frame 72...\nProcessing frame 73...\nProcessing frame 74...\nProcessing frame 75...\nProcessing frame 76...\nProcessing frame 77...\nProcessing frame 78...\nProcessing frame 79...\nProcessing frame 80...\nProcessing frame 81...\nProcessing frame 82...\nProcessing frame 83...\nProcessing frame 84...\nProcessing frame 85...\nProcessing frame 86...\nProcessing frame 87...\nProcessing frame 88...\nProcessing frame 89...\nProcessing frame 90...\nProcessing frame 91...\nProcessing frame 92...\nProcessing frame 93...\nProcessing frame 94...\nProcessing frame 95...\nProcessing frame 96...\nProcessing frame 97...\nProcessing frame 98...\nProcessing frame 99...\nProcessing frame 100...\nProcessing frame 101...\nProcessing frame 102...\nProcessing frame 103...\nProcessing frame 104...\nProcessing frame 105...\nProcessing frame 106...\nProcessing frame 107...\nProcessing frame 108...\nProcessing frame 109...\nProcessing frame 110...\nProcessing frame 111...\nProcessing frame 112...\nProcessing frame 113...\nProcessing frame 114...\nProcessing frame 115...\nProcessing frame 116...\nProcessing frame 117...\nProcessing frame 118...\nProcessing frame 119...\nProcessing frame 120...\nProcessing frame 121...\nProcessing frame 122...\nProcessing frame 123...\nProcessing frame 124...\nProcessing frame 125...\nProcessing frame 126...\nProcessing frame 127...\nProcessing frame 128...\nProcessing frame 129...\nProcessing frame 130...\nProcessing frame 131...\nProcessing frame 132...\nProcessing frame 133...\nProcessing frame 134...\nProcessing frame 135...\nProcessing frame 136...\nProcessing frame 137...\nProcessing frame 138...\nProcessing frame 139...\nProcessing frame 140...\nProcessing frame 141...\nProcessing frame 142...\nProcessing frame 143...\nProcessing frame 144...\nProcessing frame 145...\nProcessing frame 146...\nProcessing frame 147...\nProcessing frame 148...\nProcessing frame 149...\nProcessing frame 150...\nProcessing frame 151...\nProcessing frame 152...\nProcessing frame 153...\nProcessing frame 154...\nProcessing frame 155...\nProcessing frame 156...\nProcessing frame 157...\nProcessing frame 158...\nProcessing frame 159...\nProcessing frame 160...\nProcessing frame 161...\nProcessing frame 162...\nProcessing frame 163...\nProcessing frame 164...\nProcessing frame 165...\nProcessing frame 166...\nProcessing frame 167...\nProcessing frame 168...\nProcessing frame 169...\nProcessing frame 170...\nProcessing frame 171...\nProcessing frame 172...\nProcessing frame 173...\nProcessing frame 174...\nProcessing frame 175...\nProcessing frame 176...\nProcessing frame 177...\nProcessing frame 178...\nProcessing frame 179...\nProcessing frame 180...\nProcessing frame 181...\nProcessing frame 182...\nProcessing frame 183...\nProcessing frame 184...\nProcessing frame 185...\nProcessing frame 186...\nProcessing frame 187...\nProcessing frame 188...\nProcessing frame 189...\nProcessing frame 190...\nProcessing frame 191...\nProcessing frame 192...\nProcessing frame 193...\nProcessing frame 194...\nProcessing frame 195...\nProcessing frame 196...\nProcessing frame 197...\nProcessing frame 198...\nProcessing frame 199...\nProcessing frame 200...\nProcessing frame 201...\nProcessing frame 202...\nProcessing frame 203...\nProcessing frame 204...\nProcessing frame 205...\nProcessing frame 206...\nProcessing frame 207...\nProcessing frame 208...\nProcessing frame 209...\nProcessing frame 210...\nProcessing frame 211...\nProcessing frame 212...\nProcessing frame 213...\nProcessing frame 214...\nProcessing frame 215...\nProcessing frame 216...\nProcessing frame 217...\nProcessing frame 218...\nProcessing frame 219...\nProcessing frame 220...\nProcessing frame 221...\nProcessing frame 222...\nProcessing frame 223...\nProcessing frame 224...\nProcessing frame 225...\nProcessing frame 226...\nProcessing frame 227...\nProcessing frame 228...\nProcessing frame 229...\nProcessing frame 230...\nProcessing frame 231...\nProcessing frame 232...\nProcessing frame 233...\nProcessing frame 234...\nProcessing frame 235...\nProcessing frame 236...\nProcessing frame 237...\nProcessing frame 238...\nProcessing frame 239...\nProcessing frame 240...\nProcessing frame 241...\nProcessing frame 242...\nProcessing frame 243...\nProcessing frame 244...\nProcessing frame 245...\nProcessing frame 246...\nProcessing frame 247...\nProcessing frame 248...\nProcessing frame 249...\nProcessing frame 250...\nProcessing frame 251...\nProcessing frame 252...\nProcessing frame 253...\nProcessing frame 254...\nProcessing frame 255...\nProcessing frame 256...\nProcessing frame 257...\nProcessing frame 258...\nProcessing frame 259...\nProcessing frame 260...\nProcessing frame 261...\nProcessing frame 262...\nProcessing frame 263...\nProcessing frame 264...\nProcessing frame 265...\nProcessing frame 266...\nProcessing frame 267...\nProcessing frame 268...\nProcessing frame 269...\nProcessing frame 270...\nProcessing frame 271...\nProcessing frame 272...\nProcessing frame 273...\nProcessing frame 274...\nProcessing frame 275...\nProcessing frame 276...\nProcessing frame 277...\nProcessing frame 278...\nProcessing frame 279...\nProcessing frame 280...\nProcessing frame 281...\nProcessing frame 282...\nProcessing frame 283...\nProcessing frame 284...\nProcessing frame 285...\nProcessing frame 286...\nProcessing frame 287...\nProcessing frame 288...\nProcessing frame 289...\nProcessing frame 290...\nProcessing frame 291...\nProcessing frame 292...\nProcessing frame 293...\nProcessing frame 294...\nProcessing frame 295...\nProcessing frame 296...\nProcessing frame 297...\nProcessing frame 298...\nProcessing frame 299...\nProcessing frame 300...\nProcessing frame 301...\nProcessing frame 302...\nProcessing frame 303...\nProcessing frame 304...\nProcessing frame 305...\nProcessing frame 306...\nProcessing frame 307...\nProcessing frame 308...\nProcessing frame 309...\nProcessing frame 310...\nProcessing frame 311...\nProcessing frame 312...\nProcessing frame 313...\nProcessing frame 314...\nProcessing frame 315...\nProcessing frame 316...\nProcessing frame 317...\nProcessing frame 318...\nProcessing frame 319...\nProcessing frame 320...\nProcessing frame 321...\nProcessing frame 322...\nProcessing frame 323...\nProcessing frame 324...\nProcessing frame 325...\nProcessing frame 326...\nProcessing frame 327...\nProcessing frame 328...\nProcessing frame 329...\nProcessing frame 330...\nProcessing frame 331...\nProcessing frame 332...\nProcessing frame 333...\nProcessing frame 334...\nProcessing frame 335...\nProcessing frame 336...\nProcessing frame 337...\nProcessing frame 338...\nProcessing frame 339...\nProcessing frame 340...\nProcessing frame 341...\nProcessing frame 342...\nProcessing frame 343...\nProcessing frame 344...\nProcessing frame 345...\nProcessing frame 346...\nProcessing frame 347...\nProcessing frame 348...\nProcessing frame 349...\nProcessing frame 350...\nProcessing frame 351...\nProcessing frame 352...\nProcessing frame 353...\nProcessing frame 354...\nProcessing frame 355...\nProcessing frame 356...\nProcessing frame 357...\nProcessing frame 358...\nProcessing frame 359...\nProcessing frame 360...\nProcessing frame 361...\nProcessing frame 362...\nProcessing frame 363...\nProcessing frame 364...\nProcessing frame 365...\nProcessing frame 366...\nProcessing frame 367...\nProcessing frame 368...\nProcessing frame 369...\nProcessing frame 370...\nProcessing frame 371...\nProcessing frame 372...\nProcessing frame 373...\nProcessing frame 374...\nProcessing frame 375...\nProcessing frame 376...\nProcessing frame 377...\nProcessing frame 378...\nProcessing frame 379...\nProcessing frame 380...\nProcessing frame 381...\nProcessing frame 382...\nProcessing frame 383...\nProcessing frame 384...\nProcessing frame 385...\nProcessing frame 386...\nProcessing frame 387...\nProcessing frame 388...\nProcessing frame 389...\nProcessing frame 390...\nProcessing frame 391...\nProcessing frame 392...\nProcessing frame 393...\nProcessing frame 394...\nProcessing frame 395...\nProcessing frame 396...\nProcessing frame 397...\nProcessing frame 398...\nProcessing frame 399...\nProcessing frame 400...\nProcessing frame 401...\nProcessing frame 402...\nProcessing frame 403...\nProcessing frame 404...\nProcessing frame 405...\nProcessing frame 406...\nProcessing frame 407...\nProcessing frame 408...\nProcessing frame 409...\nProcessing frame 410...\nProcessing frame 411...\nProcessing frame 412...\nProcessing frame 413...\nProcessing frame 414...\nProcessing frame 415...\nProcessing frame 416...\nProcessing frame 417...\nProcessing frame 418...\nProcessing frame 419...\nProcessing frame 420...\nProcessing frame 421...\nProcessing frame 422...\nProcessing frame 423...\nProcessing frame 424...\nProcessing frame 425...\nProcessing frame 426...\nProcessing frame 427...\nProcessing frame 428...\nProcessing frame 429...\nProcessing frame 430...\nProcessing frame 431...\nProcessing frame 432...\nProcessing frame 433...\nProcessing frame 434...\nProcessing frame 435...\nProcessing frame 436...\nProcessing frame 437...\nProcessing frame 438...\nProcessing frame 439...\nProcessing frame 440...\nProcessing frame 441...\nProcessing frame 442...\nProcessing frame 443...\nProcessing frame 444...\nProcessing frame 445...\nProcessing frame 446...\nProcessing frame 447...\nProcessing frame 448...\nProcessing frame 449...\nProcessing frame 450...\nProcessing frame 451...\nProcessing frame 452...\nProcessing frame 453...\nProcessing frame 454...\nProcessing frame 455...\nProcessing frame 456...\nProcessing frame 457...\nProcessing frame 458...\nProcessing frame 459...\nProcessing frame 460...\nProcessing frame 461...\nProcessing frame 462...\nProcessing frame 463...\nProcessing frame 464...\nProcessing frame 465...\nProcessing frame 466...\nProcessing frame 467...\nProcessing frame 468...\nProcessing frame 469...\nProcessing frame 470...\nProcessing frame 471...\nProcessing frame 472...\nProcessing frame 473...\nProcessing frame 474...\nProcessing frame 475...\nProcessing frame 476...\nProcessing frame 477...\nProcessing frame 478...\nProcessing frame 479...\nProcessing frame 480...\nProcessing frame 481...\nProcessing frame 482...\nProcessing frame 483...\nProcessing frame 484...\nProcessing frame 485...\nProcessing frame 486...\nProcessing frame 487...\nProcessing frame 488...\nProcessing frame 489...\nProcessing frame 490...\nProcessing frame 491...\nProcessing frame 492...\nProcessing frame 493...\nProcessing frame 494...\nProcessing frame 495...\nProcessing frame 496...\nProcessing frame 497...\nProcessing frame 498...\nProcessing frame 499...\nProcessing frame 500...\nProcessing frame 501...\nProcessing frame 502...\nProcessing frame 503...\nProcessing frame 504...\nProcessing frame 505...\nProcessing frame 506...\nProcessing frame 507...\nProcessing frame 508...\nProcessing frame 509...\nProcessing frame 510...\nProcessing frame 511...\nProcessing frame 512...\nProcessing frame 513...\nProcessing frame 514...\nProcessing frame 515...\nProcessing frame 516...\nProcessing frame 517...\nProcessing frame 518...\nProcessing frame 519...\nProcessing frame 520...\nProcessing frame 521...\nProcessing frame 522...\nProcessing frame 523...\nProcessing frame 524...\nProcessing frame 525...\nProcessing frame 526...\nProcessing frame 527...\nProcessing frame 528...\nProcessing frame 529...\nProcessing frame 530...\nProcessing frame 531...\nProcessing frame 532...\nProcessing frame 533...\nProcessing frame 534...\nProcessing frame 535...\nProcessing frame 536...\nProcessing frame 537...\nProcessing frame 538...\nProcessing frame 539...\nProcessing frame 540...\nProcessing frame 541...\nProcessing frame 542...\nProcessing frame 543...\nProcessing frame 544...\nProcessing frame 545...\nProcessing frame 546...\nProcessing frame 547...\nProcessing frame 548...\nProcessing frame 549...\nProcessing frame 550...\nProcessing frame 551...\nProcessing frame 552...\nProcessing frame 553...\nProcessing frame 554...\nProcessing frame 555...\nProcessing frame 556...\nProcessing frame 557...\nProcessing frame 558...\nProcessing frame 559...\nProcessing frame 560...\nProcessing frame 561...\nProcessing frame 562...\nProcessing frame 563...\nProcessing frame 564...\nProcessing frame 565...\nProcessing frame 566...\nProcessing frame 567...\nProcessing frame 568...\nProcessing frame 569...\nProcessing frame 570...\nProcessing frame 571...\nProcessing frame 572...\nProcessing frame 573...\nProcessing frame 574...\nProcessing frame 575...\nProcessing frame 576...\nProcessing frame 577...\nProcessing frame 578...\nProcessing frame 579...\nProcessing frame 580...\nProcessing frame 581...\nProcessing frame 582...\nProcessing frame 583...\nProcessing frame 584...\nProcessing frame 585...\nProcessing frame 586...\nProcessing frame 587...\nProcessing frame 588...\nProcessing frame 589...\nProcessing frame 590...\nProcessing frame 591...\nProcessing frame 592...\nProcessing frame 593...\nProcessing frame 594...\nProcessing frame 595...\nProcessing frame 596...\nProcessing frame 597...\nProcessing frame 598...\nProcessing frame 599...\nProcessing frame 600...\nProcessing frame 601...\nProcessing frame 602...\nProcessing frame 603...\nProcessing frame 604...\nProcessing frame 605...\nProcessing frame 606...\nProcessing frame 607...\nProcessing frame 608...\nProcessing frame 609...\nProcessing frame 610...\nProcessing frame 611...\nProcessing frame 612...\nProcessing frame 613...\nProcessing frame 614...\nProcessing frame 615...\nProcessing frame 616...\nProcessing frame 617...\nProcessing frame 618...\nProcessing frame 619...\nProcessing frame 620...\nProcessing frame 621...\nProcessing frame 622...\nProcessing frame 623...\nProcessing frame 624...\nProcessing frame 625...\nProcessing frame 626...\nProcessing frame 627...\nProcessing frame 628...\nProcessing frame 629...\nProcessing frame 630...\nProcessing frame 631...\nProcessing frame 632...\nProcessing frame 633...\nProcessing frame 634...\nProcessing frame 635...\nProcessing frame 636...\nProcessing frame 637...\nProcessing frame 638...\nProcessing frame 639...\nProcessing frame 640...\nProcessing frame 641...\nProcessing frame 642...\nProcessing frame 643...\nProcessing frame 644...\nProcessing frame 645...\nProcessing frame 646...\nProcessing frame 647...\nProcessing frame 648...\nProcessing frame 649...\nProcessing frame 650...\nProcessing frame 651...\nProcessing frame 652...\nProcessing frame 653...\nProcessing frame 654...\nProcessing frame 655...\nProcessing frame 656...\nProcessing frame 657...\nProcessing frame 658...\nProcessing frame 659...\nProcessing frame 660...\nProcessing frame 661...\nProcessing frame 662...\nProcessing frame 663...\nProcessing frame 664...\nProcessing frame 665...\nProcessing frame 666...\nProcessing frame 667...\nProcessing frame 668...\nProcessing frame 669...\nProcessing frame 670...\nProcessing frame 671...\nProcessing frame 672...\nProcessing frame 673...\nProcessing frame 674...\nProcessing frame 675...\nProcessing frame 676...\nProcessing frame 677...\nProcessing frame 678...\nProcessing frame 679...\nProcessing frame 680...\nProcessing frame 681...\nProcessing frame 682...\nProcessing frame 683...\nProcessing frame 684...\nProcessing frame 685...\nProcessing frame 686...\nProcessing frame 687...\nProcessing frame 688...\nProcessing frame 689...\nProcessing frame 690...\nProcessing frame 691...\nProcessing frame 692...\nProcessing frame 693...\nProcessing frame 694...\nProcessing frame 695...\nProcessing frame 696...\nProcessing frame 697...\nProcessing frame 698...\nProcessing frame 699...\nProcessing frame 700...\nProcessing frame 701...\nProcessing frame 702...\nProcessing frame 703...\nProcessing frame 704...\nProcessing frame 705...\nProcessing frame 706...\nProcessing frame 707...\nProcessing frame 708...\nProcessing frame 709...\nProcessing frame 710...\nProcessing frame 711...\nProcessing frame 712...\nProcessing frame 713...\nProcessing frame 714...\nProcessing frame 715...\nProcessing frame 716...\nProcessing frame 717...\nProcessing frame 718...\nProcessing frame 719...\nProcessing frame 720...\nProcessing frame 721...\nProcessing frame 722...\nProcessing frame 723...\nProcessing frame 724...\nProcessing frame 725...\nProcessing frame 726...\nProcessing frame 727...\nProcessing frame 728...\nProcessing frame 729...\nProcessing frame 730...\nProcessing frame 731...\nProcessing frame 732...\nProcessing frame 733...\nProcessing frame 734...\nProcessing frame 735...\nProcessing frame 736...\nProcessing frame 737...\nProcessing frame 738...\nProcessing frame 739...\nProcessing frame 740...\nProcessing frame 741...\nProcessing frame 742...\nProcessing frame 743...\nProcessing frame 744...\nProcessing frame 745...\nProcessing frame 746...\nProcessing frame 747...\nProcessing frame 748...\nProcessing frame 749...\nProcessing frame 750...\nProcessing frame 751...\nProcessing frame 752...\nProcessing frame 753...\nProcessing frame 754...\nEnd of video reached.\nFinished writing 'output.mp4'.\nTotal number of unique strawberries tracked in video: 0\nAll steps (train -> eval -> video) done!\n","output_type":"stream"}],"execution_count":6}]}