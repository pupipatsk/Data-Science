{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10776052,"sourceType":"datasetVersion","datasetId":6685915}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Chatbot with Retrieval Augmented Generation (RAG)","metadata":{}},{"cell_type":"markdown","source":"## Install","metadata":{}},{"cell_type":"code","source":"!pip install faiss-cpu --quiet\n!pip install sentence-transformers --quiet\n!pip install transformers --quiet\nprint('Libraries installed.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T16:09:23.013569Z","iopub.execute_input":"2025-02-19T16:09:23.013898Z","iopub.status.idle":"2025-02-19T16:09:33.517357Z","shell.execute_reply.started":"2025-02-19T16:09:23.013875Z","shell.execute_reply":"2025-02-19T16:09:33.516331Z"}},"outputs":[{"name":"stdout","text":"Libraries installed.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Import","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport faiss\nimport numpy as np\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm.notebook import trange, tqdm\nprint('Libraries imported.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T16:09:33.518674Z","iopub.execute_input":"2025-02-19T16:09:33.518969Z","iopub.status.idle":"2025-02-19T16:09:33.524721Z","shell.execute_reply.started":"2025-02-19T16:09:33.518944Z","shell.execute_reply":"2025-02-19T16:09:33.523998Z"}},"outputs":[{"name":"stdout","text":"Libraries imported.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Load data\nDATA_DIR = \"/kaggle/input/sertis-datascience-2025-chatbot-rag-data\"\n\ndef load_data():\n    documents_df = pd.read_csv(os.path.join(DATA_DIR, \"documents.csv\"))\n    single_questions_df = pd.read_csv(os.path.join(DATA_DIR, \"single_passage_answer_questions.csv\"))\n    multi_questions_df = pd.read_csv(os.path.join(DATA_DIR, \"multi_passage_answer_questions.csv\"))\n    no_answer_questions_df = pd.read_csv(os.path.join(DATA_DIR, \"no_answer_questions.csv\"))\n    print('Data loaded successfully.')\n    return documents_df, single_questions_df, multi_questions_df, no_answer_questions_df\ndocuments_df, single_questions_df, multi_questions_df, no_answer_questions_df = load_data()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T16:09:33.526332Z","iopub.execute_input":"2025-02-19T16:09:33.526524Z","iopub.status.idle":"2025-02-19T16:09:33.578751Z","shell.execute_reply.started":"2025-02-19T16:09:33.526507Z","shell.execute_reply":"2025-02-19T16:09:33.578083Z"}},"outputs":[{"name":"stdout","text":"Data loaded successfully.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Chunking","metadata":{}},{"cell_type":"code","source":"CHUNK_SIZE = 300  # number of words per chunk\n\n# def chunk_text(text: str, chunk_size: int = 300) -> list:\n#     \"\"\"\n#     Split the text into a list of chunks, each containing up to `chunk_size` words.\n    \n#     :param text: The text to be chunked.\n#     :param chunk_size: Number of words per chunk.\n#     :return: List of text chunks.\n#     \"\"\"\n#     words = text.split()\n#     chunks = []\n#     for i in range(0, len(words), chunk_size):\n#         chunk = words[i:i+chunk_size]\n#         chunks.append(\" \".join(chunk))\n#     return chunks\n\ndef chunk_paragraphs(text: str, max_tokens: int = 300) -> list:\n    \"\"\"\n    Break text into paragraph-based chunks with a maximum token (word) limit.\n    \"\"\"\n    paragraphs = text.split(\"\\n\\n\")\n    chunks = []\n    current_chunk = []\n    current_length = 0\n    \n    for paragraph in paragraphs:\n        paragraph_length = len(paragraph.split())\n        # If adding the paragraph exceeds the limit, start a new chunk\n        if current_length + paragraph_length > max_tokens and current_chunk:\n            chunks.append(\"\\n\\n\".join(current_chunk))\n            current_chunk = []\n            current_length = 0\n        current_chunk.append(paragraph)\n        current_length += paragraph_length\n    \n    # Append any leftover text\n    if current_chunk:\n        chunks.append(\"\\n\\n\".join(current_chunk))\n    return chunks\n\n# For each document, chunk the text\nall_chunks = []\ndoc_ids = []\nfor idx, row in documents_df.iterrows():\n    print(f\"Chunking doc from {row['source_url']}\")\n    doc_id = row[\"index\"]\n    text = row[\"text\"]\n    # chunks = chunk_text(text, chunk_size=CHUNK_SIZE)\n    chunks = chunk_paragraphs(text, max_tokens=CHUNK_SIZE)\n    for chunk in chunks:\n        all_chunks.append(chunk)\n        doc_ids.append(doc_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T16:09:33.579826Z","iopub.execute_input":"2025-02-19T16:09:33.580111Z","iopub.status.idle":"2025-02-19T16:09:33.602700Z","shell.execute_reply.started":"2025-02-19T16:09:33.580048Z","shell.execute_reply":"2025-02-19T16:09:33.601960Z"}},"outputs":[{"name":"stdout","text":"Chunking doc from https://enterthegungeon.fandom.com/wiki/Bullet_Kin\nChunking doc from https://www.dropbox.com/scl/fi/ljtdg6eaucrbf1aksw5rm/c2%20-%20session%2050%20-%20underground.docx?rlkey=ioqwgkd14i5xk20i3fp38nzgs&e=1&dl=0\nChunking doc from https://bytes-and-nibbles.web.app/bytes/stici-note-part-1-planning-and-prototyping\nChunking doc from https://github.com/llmware-ai/llmware\nChunking doc from https://docs.marimo.io/recipes.html\nChunking doc from https://towardsdatascience.com/how-to-maximize-your-impact-as-a-data-scientist-3881995a9cb1\nChunking doc from https://ec.europa.eu/commission/presscorner/detail/en/QANDA_21_1683\nChunking doc from https://bg3.wiki/wiki/The_Emperor\nChunking doc from https://whattocook.substack.com/p/so-into-northern-spain\nChunking doc from https://dmtalkies.com/the-zone-of-interest-ending-explained-and-summary-2023-film/\nChunking doc from https://www.loonyparty.com/about/policy-proposals/\nChunking doc from https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/\nChunking doc from https://gleam.run/cheatsheets/gleam-for-python-users/\nChunking doc from https://towardsdatascience.com/gpt-from-scratch-with-mlx-acf2defda30e\nChunking doc from https://blog.reedsy.com/short-story/a3gstd/\nChunking doc from http://www.chakoteya.net/DoctorWho/40-1.html\nChunking doc from https://stardewvalleywiki.com/Version_History\nChunking doc from https://alanwake.fandom.com/wiki/Alan_Wake_2\nChunking doc from https://www.polygon.com/23691206/best-fantasy-books-sci-fi-2023\nChunking doc from https://arxiv.org/pdf/2404.10981\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## Embedding\n\"BAAI/bge-large-en\" \nref. https://huggingface.co/BAAI/bge-large-en","metadata":{}},{"cell_type":"code","source":"# EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n# EMBED_DIM = 384   # dimension for 'all-MiniLM-L6-v2' embeddings\nEMBEDDING_MODEL_NAME = \"BAAI/bge-large-en\"\nEMBED_DIM = 1024   # dimension for 'BAAI/bge-large-en' embeddings\n\nembedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\nprint(f\"Load model {EMBEDDING_MODEL_NAME} successfully.\")\n\n# Compute embeddings for all chunks\nprint(\"Computing embeddings for chunks...\")\nchunk_embeddings = embedding_model.encode(all_chunks, show_progress_bar=True)\nchunk_embeddings = np.array(chunk_embeddings, dtype=\"float32\")\nprint(\"Embeddings computed successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T16:09:33.603615Z","iopub.execute_input":"2025-02-19T16:09:33.603922Z","iopub.status.idle":"2025-02-19T16:10:04.834103Z","shell.execute_reply.started":"2025-02-19T16:09:33.603892Z","shell.execute_reply":"2025-02-19T16:10:04.833395Z"}},"outputs":[{"name":"stdout","text":"Load model BAAI/bge-large-en successfully.\nComputing embeddings for chunks...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/11 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a70e8992ab547e8aeca8a53773ccad7"}},"metadata":{}},{"name":"stdout","text":"Embeddings computed successfully.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## FAISS Vector Index","metadata":{}},{"cell_type":"code","source":"index = faiss.IndexFlatL2(EMBED_DIM)\nindex.add(chunk_embeddings)\nprint(f\"FAISS index size: {index.ntotal}\")\n\n# Keep a lookup of chunk_id -> text\nchunk_id_to_text = {i: chunk for i, chunk in enumerate(all_chunks)}\nchunk_id_to_docid = {i: doc_ids[i] for i in range(len(doc_ids))}","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-19T16:10:04.834861Z","iopub.execute_input":"2025-02-19T16:10:04.835091Z","iopub.status.idle":"2025-02-19T16:10:04.841591Z","shell.execute_reply.started":"2025-02-19T16:10:04.835052Z","shell.execute_reply":"2025-02-19T16:10:04.840711Z"}},"outputs":[{"name":"stdout","text":"FAISS index size: 321\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## Generation Model\n\"google/flan-t5-large\"\nref. https://huggingface.co/google/flan-t5-large","metadata":{}},{"cell_type":"code","source":"# GENERATION_MODEL_NAME = \"google/flan-t5-small\"\nGENERATION_MODEL_NAME = \"google/flan-t5-large\"\ntokenizer = AutoTokenizer.from_pretrained(GENERATION_MODEL_NAME)\ngen_model = AutoModelForSeq2SeqLM.from_pretrained(GENERATION_MODEL_NAME)\ndef generate_answer(context: str, question: str, \n                    max_length: int = 128) -> str:\n    \"\"\"\n    Use a seq2seq model to generate an answer from the given context and question.\n    \n    :param context: Relevant textual context.\n    :param question: The question posed by the user.\n    :param max_length: Maximum tokens for generation.\n    :return: Generated answer string.\n    \"\"\"\n    # Create a simple prompt\n    prompt = (\n        \"You are an AI assistant. Provide a helpful, concise answer.\\n\\n\"\n        \"Context:\\n\"\n        f\"{context}\\n\\n\"\n        \"Question:\\n\"\n        f\"{question}\\n\\n\"\n        \"Instructions: Provide a direct and accurate answer. Avoid speculation.\\n\"\n        \"Answer:\"\n    )\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n    with torch.no_grad():\n        outputs = gen_model.generate(**inputs, max_length=max_length)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-19T16:10:04.842387Z","iopub.execute_input":"2025-02-19T16:10:04.842647Z","iopub.status.idle":"2025-02-19T16:10:06.808024Z","shell.execute_reply.started":"2025-02-19T16:10:04.842615Z","shell.execute_reply":"2025-02-19T16:10:06.807324Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## RAG Retrieval","metadata":{}},{"cell_type":"code","source":"def retrieve_top_k(query: str, top_k: int = 3, distance_threshold: float = 1.0) -> list:\n    \"\"\"\n    Retrieve the top_k chunks from the FAISS index for a given query.\n    \n    :param query: The user query/question.\n    :param top_k: Number of chunks to retrieve.\n    :return: A list of (score, chunk_text) sorted by ascending distance.\n    \"\"\"\n    query_embedding = embedding_model.encode([query]).astype(\"float32\")\n    distances, indices = index.search(query_embedding, top_k)\n    \n    # If best distance is above threshold, we conclude no answer is found\n    if distances[0][0] > distance_threshold:\n        return \"No answer found in the knowledge base.\"\n        \n    results = []\n    for dist, idx in zip(distances[0], indices[0]):\n        chunk_text = chunk_id_to_text[idx]\n        results.append((dist, chunk_text))\n    return results\n\nGUARDRAILS_KEYWORDS = [\n    \"politics\",\n    \"terrorism\",\n    \"violence\",\n    \"hate speech\",\n    \"abuse\",\n    \"amphetamines\",\n]\ndef is_guarded(query: str) -> bool:\n    \"\"\"\n    Check if the query contains guarded keywords.\n    Naive keyword-based.\n    \n    :param query: The input user query.\n    :return: True if the query is guarded (contains certain sensitive or misused terms), else False.\n    \"\"\"\n    lower_query = query.lower()\n    for keyword in GUARDRAILS_KEYWORDS:\n        if keyword in lower_query:\n            return True\n    return False\n\nTOP_K = 3         # number of retrieved chunks\ndef rag_pipeline(question: str, top_k: int = TOP_K) -> str:\n    \"\"\"\n    End-to-end RAG pipeline to answer a question:\n    1) Guardrail checks\n    2) Retrieve relevant chunks\n    3) Generate answer\n    \n    :param question: Input user question.\n    :param top_k: Number of chunks to retrieve.\n    :return: Final answer string.\n    \"\"\"\n    # 1) Guardrail\n    if is_guarded(question):\n        return \"I’m sorry, but I can’t help with that.\"\n    \n    # 2) Retrieval\n    top_chunks = retrieve_top_k(question, top_k=top_k)\n    # Concatenate all relevant chunks into a single context\n    combined_context = \"\\n\".join([chunk for _, chunk in top_chunks])\n    \n    # 3) Generation\n    answer = generate_answer(combined_context, question)\n    return answer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-19T16:10:06.809821Z","iopub.execute_input":"2025-02-19T16:10:06.810091Z","iopub.status.idle":"2025-02-19T16:10:06.817466Z","shell.execute_reply.started":"2025-02-19T16:10:06.810040Z","shell.execute_reply":"2025-02-19T16:10:06.816417Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Quick test on single-passage questions\n    print(\"\\n===== Example: Single-Passage Question =====\")\n    sample_single = single_questions_df.sample(1).iloc[0]\n    question_sp = sample_single[\"question\"]\n    true_answer_sp = sample_single[\"answer\"]\n    print(f\"Question: {question_sp}\")\n    predicted_answer_sp = rag_pipeline(question_sp, TOP_K)\n    print(f\"Predicted Answer: {predicted_answer_sp}\")\n    print(f\"True Answer: {true_answer_sp}\")\n\n    # Quick test on multi-passage questions\n    print(\"\\n===== Example: Multi-Passage Question =====\")\n    sample_multi = multi_questions_df.sample(1).iloc[0]\n    question_mp = sample_multi[\"question\"]\n    true_answer_mp = sample_multi[\"answer\"]\n    print(f\"Question: {question_mp}\")\n    predicted_answer_mp = rag_pipeline(question_mp, TOP_K)\n    print(f\"Predicted Answer: {predicted_answer_mp}\")\n    print(f\"True Answer: {true_answer_mp}\")\n\n    # Quick test on no-answer questions\n    print(\"\\n===== Example: No-Answer Question =====\")\n    sample_noans = no_answer_questions_df.sample(1).iloc[0]\n    question_noans = sample_noans[\"question\"]\n    print(f\"Question: {question_noans}\")\n    predicted_answer_noans = rag_pipeline(question_noans, TOP_K)\n    print(f\"Predicted Answer: {predicted_answer_noans} (No known answer in docs)\")\n    \n    # Test guardrail\n    print(\"\\n===== Example: Guardrail Trigger =====\")\n    # blocked_query = \"What are the politics of this country?\"\n    blocked_query = \"How to cook amphetamines?\"\n    print(f\"Question: {blocked_query}\")\n    blocked_answer = rag_pipeline(blocked_query)\n    print(f\"Answer: {blocked_answer}\")\n\n    print(\"\\nAll steps completed!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-19T16:10:06.818668Z","iopub.execute_input":"2025-02-19T16:10:06.818997Z","iopub.status.idle":"2025-02-19T16:10:20.015028Z","shell.execute_reply.started":"2025-02-19T16:10:06.818965Z","shell.execute_reply":"2025-02-19T16:10:20.014149Z"}},"outputs":[{"name":"stdout","text":"\n===== Example: Single-Passage Question =====\nQuestion: What is the policy on Tai Chi?\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"705854cb77434c87a295e844a1114e1c"}},"metadata":{}},{"name":"stdout","text":"Predicted Answer: Model distillation can actually improve accuracy in domain-specific tasks while making a smaller model.\nTrue Answer: In order to calm down the passions and stresses currently exhibited in Parliament, the Loony Party would make all M.P’s have half an hours compulsory Tai chi everyday.\n\n===== Example: Multi-Passage Question =====\nQuestion: Which books contain multiple narratives?\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9392c97122fc49ed9ff97884e6b9bd68"}},"metadata":{}},{"name":"stdout","text":"Predicted Answer: The Scarlet Alchemist is that it is a book that is a mix of fiction and nonfiction.\nTrue Answer: The books 'Same Bed Different Dreams', 'EVERY DROP IS A MAN’S NIGHTMARE', 'The Museum of Human History', 'Witch King', and 'The Terraformers' each use multiple narratives.\n\n===== Example: No-Answer Question =====\nQuestion: What caliber is the bullet of light?\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81e4d993549546c3bb37fab7d9502a59"}},"metadata":{}},{"name":"stdout","text":"Predicted Answer: Bullet Kin (No known answer in docs)\n\n===== Example: Guardrail Trigger =====\nQuestion: How to cook amphetamines?\nAnswer: I’m sorry, but I can’t help with that.\n\nAll steps completed!\n","output_type":"stream"}],"execution_count":19}]}